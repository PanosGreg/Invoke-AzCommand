

New feature ideas for the Invoke-AzCommand
------------------------------------------

Ideas as of: 07-Nov-2023

1) -AsJob parameter    -- DONE
   (Look at the spinner module for this)
   
2) PSDrive with the logs
   - Input.Script
   - Input.Arguments
   - Output.Results
   - Output.Errors
   - Timestamp
   - TargetVMs
   - AzUserName
   - TotalDuration
   - Unique ID of command
   - Reason (for Jira tickets)
   
   Get an idea from AWS SSM Run Command
   and the History of commands
   
   May even use SQLite for this.

   Actually, this might be a whole new module on its own.
   On the Invoke-AzCommand, just fire an event with all the relevant data (see above)
   and then that new module, will create the PS drive, register to that queue
   and also update the SQLite DB to persist the data.
   
3) PSDrive with Configuration
   OR -AzSessionOption [AzSessionOption]
   and then New-AzSessionOption
   like New-PSSessionOption or New-PesterConfiguration
   
   And add everything in the config
   - Serialization type: xml|json
   - Serialization Depth
   - Compression on/off
   - Add Az Properties on/off
   - Format files to load by default for out-string (see Get-CompressionOutput)
   - Default Progress status message
   
   The idea here is, to add all options
   that you would have as ** feature flags **
   
4) -ReferenceModule and -ReferenceFunction parameters
   To copy and load the module(s)/function(s)
   onto the remote machine, so the user can use them
   inside his scriptblock.
   Load them up in the runspace job as context.
   
5) Encryption of user code and args as well as returned results
   This might need the extra Encipher-String module
   instead of having that logic in the AzCommand module

6) -Credential to run the remote command as that user   --- DONE
   As-in spawn a process in the remote VM with RunAs
   
   I need to thank Jordan Borean for his great work on this function.
   The actual source for this is here: https://gist.github.com/jborean93/3c148df03545023c671ddefb2d2b5ffc
   He's pretty good with C#.

6a) If I add the credential parameter, then it makes sense to   --- DONE
    also implement encryption. Since we don't want the user/pass
    to be send in clear over the channel.

7) -Reason or -Description param [optional], so user can add any context
   on why/what he's doing.
   Most common use-case would be to put the Jira ticket ID in that
   param. So you can trace it back to a ticket or add the output log to that ticket.
   
8) [maybe] Add a copy functionality from local to the remote VMs
   just like Copy-Item that can work through PSSessions, or just
   like SSH where you can use it to copy files via SCP (secure copy)

   I could do this either directly, as-in spin up a server locally
   and then have the VM connect to that server, or vice versa (as-in
   spin up the server on the VM and have your local connect to it, but
   that needs the VM to be publicly available). Another way is to have
   an intermediate. Like S3 or GitHub or Azure NAS drive or anything else.

   This functionality could probably be done on a separate module perhaps.
   So maybe not in the same module as Invoke-AzCommand but as an additional
   module, just like the idea for the logging

9) Add a PreScript (or Before) and a PostScript (or After), so the user can do
   any needed setup and then teardown apart from his scriptblock.

   The idea is very much the same just like the Before/After blocks in Pester,
   or the Begin/End block in foreach-object, or just like the AWS SSM document
   AWS-RunPatchBaselineWithHooks where you have a pre and a post script.

   For example, in the Before block you may need to stop and disable the chef client
   scheduled task and then on the After block enable it back again.

   Similarly you may need to drain a server first (like remove it from LB forwarding
   list or failover a DB, etc), and once you're done, undrain it.

   And then keep that same Before/After and use it on many different Invoke-AzCommand scripts.
    
10)Make the Before/After scriptblocks (see above) to be able to set on a per VM basis.
   So that you can have different scripblocks for before and/or after for each VM.
   For example. You could set the proxy which is different on STG, PRD or DEV.
   Even though all of those VMs could be on the same subscription.
   Another one is, you may want to drain the server but the drain process is different
   based on the role, as-in if it's a web server or a db server, no matter the environment.
   Both of those 2 use-cases can be based on the VM name, the env prefix stg/prd/dev,
   as well as the web,db role suffix.
   So make it possible to have different before/after scriptblocks for each VM

11)Be able to have different scriptblocks that will run with different user accounts.
   The way I handle the impersonation, it's done through a function that uses P/Invoke and
   it runs seemlessly (unlike Start-Process for example which opens a new window).
   So what we could do is be able to run individual script parts with different creds.
   For example, create a .Net type @{[scriptblock]Scriptblock;[pscredential]$Credential}
   and then have an array of that. So the scriptblocks will run in sequence on the VM
   but each scriptblock can run under a different account.
   For example, first you need to failover the DB, so you do that with an account that
   has access to SQL Server, then you need to make a change in the network, so you run
   that with a different account that has domain access, and finally let's say you need
   to make a change on the local VM, so you can run that with the default System account.
   In this scenario you've used 3 different accounts to do your work.

12)This is the big one - Alternate data transfer modes.
   The idea is to spin up a web server on the local machine and have the VM connect to that
   so you can send data back without the limit from Azure VM Agent service (or the limit
   from the AWS SSM command). Now to do that, you'll need to have a reachable IP on your
   machine so that the server can connect to. A standard approach is to run the
   Invoke-AzCommand from a cloud VM, and then put a Load Balancer in front of it which
   will be front-facing and publicly available. Maybe not available to the whole internet
   but perhaps only to the VMs of your environments.
   The local server could be done with [HTTPListener] so that you don't have any dependencies
   (as-in keep it simple, and don't use Pode or Universal Dashboard).
   And then have a parameter like so: -TransferMode (withAzure|withHttpDirect).
   Now another idea is to also give a 3rd option: withMessages
   On that 3rd option you could use RabbitMQ on your machine and then have the RMQ client
   (that's essentially a .dll) connect to the RAbbitMQ server. Again the RMQ service will
   need to be reachable so the use of a front-facing Load Balancer is still needed.
   And then have a queue, your mahcine will subscribe to that queue, and the remote VM
   will publish to that queue.

   Think what Mark Russinovich did when he made psexec. He copies the executable to the
   remote server, creates a new service on the fly and the runs that service, so that he
   can run the user's code under the account he needs. And once done, he unregisters the
   service and deletes it. And all of that are done on the spot. So why not then do a
   similar thing, in regards to the on-the-fly setup for a connection. Like spin a web
   service, have the client conenct to it and then use that communication channel.

   Another thing to consider, is you can have a Dynamic Parameter in Invoke-AzCommand
   that checks if the TransferMode is let's say withHttpDirect, then it adds extra
   parameters like the IP to connect to and maybe some credentials needed or API key.
   Cause the target VM does not know where it needs to connect obviously.

   And this approach will a) remove the data transfer limit and b) allow for real-time
   data to be send back to your machine, cause now we don't have to wait for the command
   to finish to get the results. c) we could use this to send files to the VMs as well
   so that we don't need to use an intermediary like S3 or Azure Blob or anything else.
   That way you can deploy things easily. So then each VM won't have to have access to the file
   storage, but rather only your machine will need that.




DONE
-----

1) Remote ErrorRecord re-hydration / re-creation on the local machine
2) AsJob parameter to run the public function as a background job
3) Fe-factor "foreach parallel with progress bars" as to remove boilerplate
   from the public function
4) Add option to run as a different user / impresonate a user on the remote
   machine
5) Encrypt credentials provided by user through the Credential parameter
   (and obviously decrypt them remotely so they can be used)
   Use the Azure subscription as the encryption/decryption key
6) Add the option for the end-user to pass in either positional or named
   parameters
7) Remove the comments from the helper functions that I use remotely
   to reduce the string length
8) Add 2 extra properties on all output objects (including error records)
9) Don't show our extra properties by default.
10)Re-write the front-end readme to make the module more accessible.


Priorities
-----------

Since there's so many things I can do to extend the functionality
Here's a preliminary priority list.
1) add ReferenceModule to load a local module remotely (since we have a 4MB limit on input, which is large enough)  
   (this would be similar in concept to Add-Type -ReferencedAssemblies ...)
2) Add logging through custom PS drive. This should be an external module. This is a substantial effort.
3) Add copy functionality. This is an external module or function that will use the Invoke-AzCommand (dogfooding)
4) Add data channel through REST API server. This is a substantial effort.
5) Write encryption/decryption module and have option to use it.
   (this actually could be done with the ReferenceModule option, so user can do it on his own)


Even more ideas
----------------

a) Profiling
Profile the user's code during runtime, and save the trace in Azure Storage.
Then when you collect the results, store the traces locally in the custom PS Drive,
so the user can go check them out.

Talk to Jakub, about the possibility to extend the profiler module.
Track network and memory stats before and after each command. The idea is to send the tracing
results into a log platform (ex. Prometheus+Grafana) and be able to see the resources used
on the commands that were run in the past. Things like how many network calls were done,
or how many bytes were sent/received, or how much disk space was used on commands,
or how much memory was used, or which .NET classes are using how much memory.

b) Logging
Create a custom event provider on the target VMs, and log everything there upon each execution.
So that when an engineer needs to check what happened, he can track it back through the Event Viewer.

